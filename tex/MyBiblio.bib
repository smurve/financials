@article{Ritter12,
	author = {Gordon Ritter},
	title = {Deep Hedging},
	journaltitle = {IEEE 2012},
	date = {12.11.2009},
}


@article{Jiang2017,
	archivePrefix = {arXiv},
	arxivId = {1706.10059},
	author = {Jiang, Zhengyao and Xu, Dixing and Liang, Jinjun},
	eprint = {1706.10059},
	file = {:Users/wgiersche/Library/Application Support/Mendeley Desktop/Downloaded/Jiang, Xu, Liang - 2017 - A Deep Reinforcement Learning Framework for the Financial Portfolio Management Problem.pdf:pdf},
	keywords = {algorithmic trading,bitcoin,convolutional neural networks,cryptocur-,deep learning,long short-term memory,machine learning,portfolio management,quantitative finance,recurrent neural net-,reinforcement learning,rency,works},
	mendeley-groups = {Finance,DeepRL},
	pages = {1--31},
	title = {{A Deep Reinforcement Learning Framework for the Financial Portfolio Management Problem}},
	url = {http://arxiv.org/abs/1706.10059},
	year = {2017}
}


@article{LSTM97,
	author = {Hochreiter, Sepp and Schmidhuber, JÃ¼rgen},
	title = {Long Short-Term Memory},
	journal = {Neural Computation},
	volume = {9},
	number = {8},
	pages = {1735-1780},
	year = {1997},
	doi = {10.1162/neco.1997.9.8.1735},
	URL = { 
	https://doi.org/10.1162/neco.1997.9.8.1735
	},
	eprint = { 
	https://doi.org/10.1162/neco.1997.9.8.1735
	}
}


@article{Lillicrap2015,
	abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
	archivePrefix = {arXiv},
	arxivId = {1509.02971},
	author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
	eprint = {1509.02971},
	file = {:Users/wgiersche/Library/Application Support/Mendeley Desktop/Downloaded/Lillicrap et al. - 2015 - Continuous control with deep reinforcement learning(2).pdf:pdf},
	month = {sep},
	title = {{Continuous control with deep reinforcement learning}},
	url = {http://arxiv.org/abs/1509.02971},
	year = {2015}
}

